{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler\n",
    "from env import get_db_url\n",
    "import wrangle\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a customer analyst, I want to know who has spent the most money with us over their lifetime. I have monthly charges and tenure, so I think I will be able to use those two attributes as features to estimate total_charges. I need to do this within an average of \\$5.00 per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = get_db_url('telco_churn')\n",
    "df = pd.read_sql('''\n",
    "SELECT customer_id, tenure, monthly_charges, total_charges\n",
    "FROM customers\n",
    "where contract_type_id = 3\n",
    "''', url)\n",
    "df = wrangle.drop_nulls(wrangle.wrangle_telco(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create split_scale.py that will contain the functions that follow. Each scaler function should create the object, fit and transform both train and test. They should return the scaler, train df scaled, test df scaled. Be sure your indices represent the original indices from train/test, as those represent the indices from the original dataframe. Be sure to set a random state where applicable for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_my_data(x, y, train_pct = .80):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, \n",
    "                                                        train_size = train_pct,\n",
    "                                                        random_state = 123)\n",
    "    return [train_x, train_y], [test_x, test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(train, test):\n",
    "    scaler_object = StandardScaler(copy=True, \n",
    "                                   with_mean=True, \n",
    "                                   with_std=True).fit(train) \n",
    "    scaled_train = apply_object(train, scaler_object)\n",
    "    scaled_test  = apply_object(test,  scaler_object)\n",
    "    return scaler_object, scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_inverse(array, scaler_object):\n",
    "    return pd.DataFrame(scaler_object.inverse_transform(array), columns=array.columns.values).set_index([array.index.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_scaler(train, test, quantiles = 100):\n",
    "    scaler_object = QuantileTransformer(n_quantiles=quantiles, \n",
    "                                 output_distribution='uniform', \n",
    "                                 random_state=123, \n",
    "                                 copy=True).fit(train)\n",
    "    scaled_train = apply_object(train, scaler_object)\n",
    "    scaled_test  = apply_object(test,  scaler_object)\n",
    "    return scaler_object, scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_scaler(train, test ,method = 'yeo-johnson'):\n",
    "    scaler_object = PowerTransformer(method = method, \n",
    "                                     standardize = False, \n",
    "                                     copy = True).fit(train)\n",
    "    scaled_train = apply_object(train, scaler_object)\n",
    "    scaled_test  = apply_object(test,  scaler_object)\n",
    "    return scaler_object, scaled_train, scaled_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(train, test):\n",
    "    scaler_object = MinMaxScaler(copy=True, \n",
    "                                 feature_range=(0,1)).fit(train)\n",
    "    scaled_train = apply_object(train, scaler_object)\n",
    "    scaled_test  = apply_object(test,  scaler_object)\n",
    "    return scaler_object, scaled_train, scaled_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_robust_scaler(train, test, quantiles = (25.0, 75.0)):\n",
    "    scaler_object = RobustScaler(quantile_range = quantiles, \n",
    "                                 copy = True, with_centering=True, \n",
    "                                 with_scaling = True).fit(train)\n",
    "    scaled_train = apply_object(train, scaler_object)\n",
    "    scaled_test  = apply_object(test,  scaler_object)\n",
    "    return scaler_object, scaled_train, scaled_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_object(x, scaler_object):\n",
    "    return pd.DataFrame(scaler_object.transform(x), \n",
    "                        columns=x.columns.values).set_index([x.index.values])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = df[['customer_id']] \n",
    "x = df[['tenure', 'monthly_charges']]\n",
    "y = df[['total_charges']]\n",
    "train, test = split_my_data(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "scaler_x, _,_ = standard_scaler(train[0],test[0])\n",
    "print(scaler_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "scaler_y, _, _ = standard_scaler(train[0],test[0])\n",
    "print(scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x == scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
